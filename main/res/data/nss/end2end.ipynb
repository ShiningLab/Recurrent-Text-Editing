{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End2End Moldes\n",
    "+ Naive GRU RNN\n",
    "+ Naive LSTM RNN\n",
    "+ Bi-directional GRU RNN\n",
    "+ Bi-directional LSTM RNN\n",
    "+ Bi-directional GRU RNN with Attention\n",
    "+ Bi-directional LSTM RNN with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "+ Encoder and Decoder have separate embedding layers\n",
    "+ There are two training methods, namely, online and offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "__author__ = 'Shining'\n",
    "__email__ = 'mrshininnnnn@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency\n",
    "# public\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "# private\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "num_size = 100\n",
    "seq_len = 5\n",
    "data_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nss/num_size_100/seq_len_5/data_size_10000'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load path\n",
    "indir = 'nss'\n",
    "indir = os.path.join(indir, \n",
    "                     'num_size_{}'.format(num_size), \n",
    "                     'seq_len_{}'.format(seq_len), \n",
    "                     'data_size_{}'.format(data_size))\n",
    "indir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end2end/num_size_100/seq_len_5/data_size_10000'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save path\n",
    "outdir = 'end2end'\n",
    "\n",
    "outdir = os.path.join(outdir, \n",
    "                      'num_size_{}'.format(num_size), \n",
    "                      'seq_len_{}'.format(seq_len), \n",
    "                      'data_size_{}'.format(data_size))\n",
    "if not os.path.exists(outdir): \n",
    "    os.makedirs(outdir)\n",
    "outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw dataset\n",
    "raw_train_xs = load_txt(os.path.join(indir, 'train_x.txt'))\n",
    "raw_train_ys = load_txt(os.path.join(indir, 'train_y.txt'))\n",
    "raw_val_xs = load_txt(os.path.join(indir, 'val_x.txt'))\n",
    "raw_val_ys = load_txt(os.path.join(indir, 'val_y.txt'))\n",
    "raw_test_xs = load_txt(os.path.join(indir, 'test_x.txt'))\n",
    "raw_test_ys = load_txt(os.path.join(indir, 'test_y.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample size 7000\n",
      "train label size 7000\n",
      "val sample size 1500\n",
      "val label size 1500\n",
      "test sample size 1500\n",
      "test label size 1500\n"
     ]
    }
   ],
   "source": [
    "# check data size\n",
    "print('train sample size', len(raw_train_xs))\n",
    "print('train label size', len(raw_train_ys))\n",
    "print('val sample size', len(raw_val_xs))\n",
    "print('val label size', len(raw_val_ys))\n",
    "print('test sample size', len(raw_test_xs))\n",
    "print('test label size', len(raw_test_ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: 10 26 5 23 79\n",
      "tgt: 5 10 23 26 79\n",
      "\n",
      "src: 54 91 94 6 50\n",
      "tgt: 6 50 54 91 94\n",
      "\n",
      "src: 42 65 1 28 17\n",
      "tgt: 1 17 28 42 65\n",
      "\n",
      "src: 11 90 26 38 25\n",
      "tgt: 11 25 26 38 90\n",
      "\n",
      "src: 30 12 25 9 37\n",
      "tgt: 9 12 25 30 37\n",
      "\n",
      "src: 99 34 6 80 70\n",
      "tgt: 6 34 70 80 99\n",
      "\n",
      "src: 77 41 30 5 21\n",
      "tgt: 5 21 30 41 77\n",
      "\n",
      "src: 2 67 2 28 13\n",
      "tgt: 2 2 13 28 67\n",
      "\n",
      "src: 20 80 70 81 24\n",
      "tgt: 20 24 70 80 81\n",
      "\n",
      "src: 72 38 64 5 85\n",
      "tgt: 5 38 64 72 85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "for i in range(-10, 0, 1):\n",
    "    print('src:', raw_train_xs[i])\n",
    "    print('tgt:', raw_train_ys[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white space tokenization\n",
    "train_xs = white_space_tokenizer(raw_train_xs)\n",
    "train_ys = white_space_tokenizer(raw_train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('29', 391), ('9', 389), ('52', 388), ('25', 386), ('55', 383), ('87', 383), ('60', 383), ('47', 379), ('84', 378), ('83', 378), ('75', 377), ('74', 377), ('59', 375), ('27', 374), ('5', 373), ('67', 371), ('20', 369), ('33', 369), ('58', 368), ('31', 365), ('41', 365), ('45', 365), ('51', 364), ('98', 364), ('37', 364), ('8', 364), ('91', 364), ('46', 363), ('81', 362), ('89', 360), ('65', 359), ('57', 358), ('22', 358), ('79', 358), ('70', 357), ('76', 356), ('71', 354), ('23', 354), ('97', 354), ('88', 354), ('49', 353), ('85', 353), ('32', 352), ('66', 352), ('15', 352), ('44', 351), ('19', 350), ('94', 350), ('28', 349), ('1', 349), ('43', 349), ('30', 349), ('17', 347), ('4', 347), ('40', 345), ('82', 345), ('26', 345), ('95', 345), ('42', 344), ('48', 343), ('56', 343), ('11', 343), ('7', 342), ('35', 342), ('54', 342), ('80', 341), ('99', 341), ('62', 341), ('18', 339), ('14', 339), ('16', 339), ('13', 338), ('96', 337), ('72', 337), ('3', 337), ('69', 336), ('36', 335), ('6', 335), ('12', 334), ('93', 334), ('38', 334), ('78', 334), ('64', 333), ('10', 333), ('90', 332), ('61', 331), ('34', 331), ('92', 331), ('21', 330), ('68', 329), ('0', 327), ('50', 324), ('63', 322), ('86', 322), ('73', 321), ('2', 320), ('24', 317), ('77', 312), ('53', 311), ('39', 308)]\n"
     ]
    }
   ],
   "source": [
    "# vocabulary frequency distribution\n",
    "counter = Counter()\n",
    "for x in train_xs:\n",
    "    counter.update(x)\n",
    "    \n",
    "print(counter.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']\n"
     ]
    }
   ],
   "source": [
    "src_vocab_list = sorted(counter.keys())\n",
    "print(src_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '0': 1, '1': 2, '10': 3, '11': 4, '12': 5, '13': 6, '14': 7, '15': 8, '16': 9, '17': 10, '18': 11, '19': 12, '2': 13, '20': 14, '21': 15, '22': 16, '23': 17, '24': 18, '25': 19, '26': 20, '27': 21, '28': 22, '29': 23, '3': 24, '30': 25, '31': 26, '32': 27, '33': 28, '34': 29, '35': 30, '36': 31, '37': 32, '38': 33, '39': 34, '4': 35, '40': 36, '41': 37, '42': 38, '43': 39, '44': 40, '45': 41, '46': 42, '47': 43, '48': 44, '49': 45, '5': 46, '50': 47, '51': 48, '52': 49, '53': 50, '54': 51, '55': 52, '56': 53, '57': 54, '58': 55, '59': 56, '6': 57, '60': 58, '61': 59, '62': 60, '63': 61, '64': 62, '65': 63, '66': 64, '67': 65, '68': 66, '69': 67, '7': 68, '70': 69, '71': 70, '72': 71, '73': 72, '74': 73, '75': 74, '76': 75, '77': 76, '78': 77, '79': 78, '8': 79, '80': 80, '81': 81, '82': 82, '83': 83, '84': 84, '85': 85, '86': 86, '87': 87, '88': 88, '89': 89, '9': 90, '90': 91, '91': 92, '92': 93, '93': 94, '94': 95, '95': 96, '96': 97, '97': 98, '98': 99, '99': 100}\n"
     ]
    }
   ],
   "source": [
    "# soruce vocabulary dictionary\n",
    "src_vocab2idx_dict = dict()\n",
    "src_vocab2idx_dict['<pad>'] = 0 # to pad sequence length\n",
    "\n",
    "i = len(src_vocab2idx_dict)\n",
    "for token in src_vocab_list:\n",
    "    src_vocab2idx_dict[token] = i\n",
    "    i += 1\n",
    "\n",
    "print(src_vocab2idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('29', 391), ('9', 389), ('52', 388), ('25', 386), ('55', 383), ('87', 383), ('60', 383), ('47', 379), ('84', 378), ('83', 378), ('75', 377), ('74', 377), ('59', 375), ('27', 374), ('5', 373), ('67', 371), ('20', 369), ('33', 369), ('58', 368), ('31', 365), ('41', 365), ('45', 365), ('51', 364), ('98', 364), ('37', 364), ('8', 364), ('91', 364), ('46', 363), ('81', 362), ('89', 360), ('65', 359), ('22', 358), ('57', 358), ('79', 358), ('70', 357), ('76', 356), ('71', 354), ('23', 354), ('97', 354), ('88', 354), ('49', 353), ('85', 353), ('32', 352), ('66', 352), ('15', 352), ('44', 351), ('19', 350), ('94', 350), ('1', 349), ('28', 349), ('43', 349), ('30', 349), ('17', 347), ('4', 347), ('40', 345), ('82', 345), ('26', 345), ('95', 345), ('42', 344), ('48', 343), ('56', 343), ('11', 343), ('7', 342), ('35', 342), ('54', 342), ('80', 341), ('99', 341), ('62', 341), ('18', 339), ('14', 339), ('16', 339), ('13', 338), ('72', 337), ('96', 337), ('3', 337), ('69', 336), ('36', 335), ('6', 335), ('12', 334), ('93', 334), ('38', 334), ('78', 334), ('64', 333), ('10', 333), ('90', 332), ('61', 331), ('34', 331), ('92', 331), ('21', 330), ('68', 329), ('0', 327), ('50', 324), ('63', 322), ('86', 322), ('73', 321), ('2', 320), ('24', 317), ('77', 312), ('53', 311), ('39', 308)]\n"
     ]
    }
   ],
   "source": [
    "# target vocabulary frequency distribution\n",
    "counter = Counter()\n",
    "for y in train_ys:\n",
    "    counter.update(y)\n",
    "\n",
    "print(counter.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']\n"
     ]
    }
   ],
   "source": [
    "tgt_vocab_list = sorted(counter.keys())\n",
    "print(tgt_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<s>': 1, '</s>': 2, '0': 3, '1': 4, '10': 5, '11': 6, '12': 7, '13': 8, '14': 9, '15': 10, '16': 11, '17': 12, '18': 13, '19': 14, '2': 15, '20': 16, '21': 17, '22': 18, '23': 19, '24': 20, '25': 21, '26': 22, '27': 23, '28': 24, '29': 25, '3': 26, '30': 27, '31': 28, '32': 29, '33': 30, '34': 31, '35': 32, '36': 33, '37': 34, '38': 35, '39': 36, '4': 37, '40': 38, '41': 39, '42': 40, '43': 41, '44': 42, '45': 43, '46': 44, '47': 45, '48': 46, '49': 47, '5': 48, '50': 49, '51': 50, '52': 51, '53': 52, '54': 53, '55': 54, '56': 55, '57': 56, '58': 57, '59': 58, '6': 59, '60': 60, '61': 61, '62': 62, '63': 63, '64': 64, '65': 65, '66': 66, '67': 67, '68': 68, '69': 69, '7': 70, '70': 71, '71': 72, '72': 73, '73': 74, '74': 75, '75': 76, '76': 77, '77': 78, '78': 79, '79': 80, '8': 81, '80': 82, '81': 83, '82': 84, '83': 85, '84': 86, '85': 87, '86': 88, '87': 89, '88': 90, '89': 91, '9': 92, '90': 93, '91': 94, '92': 95, '93': 96, '94': 97, '95': 98, '96': 99, '97': 100, '98': 101, '99': 102}\n"
     ]
    }
   ],
   "source": [
    "# target vocabulary dictionary\n",
    "tgt_vocab2idx_dict = dict()\n",
    "tgt_vocab2idx_dict['<pad>'] = 0 # to pad sequence length\n",
    "tgt_vocab2idx_dict['<s>'] = 1 # to mark the start of a sequence\n",
    "tgt_vocab2idx_dict['</s>'] = 2 # to mark the end of a sequence\n",
    "\n",
    "i = len(tgt_vocab2idx_dict)\n",
    "for token in tgt_vocab_list:\n",
    "    tgt_vocab2idx_dict[token] = i\n",
    "    i += 1\n",
    "\n",
    "print(tgt_vocab2idx_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: 3 8 1 4 5\n",
      "tgt: 1 3 4 5 8\n",
      "\n",
      "src: 0 0 6 4 9\n",
      "tgt: 0 0 4 6 9\n",
      "\n",
      "src: 5 3 3 9 0\n",
      "tgt: 0 3 3 5 9\n",
      "\n",
      "src: 1 9 8 4 4\n",
      "tgt: 1 4 4 8 9\n",
      "\n",
      "src: 0 0 4 5 0\n",
      "tgt: 0 0 0 4 5\n",
      "\n",
      "src: 5 6 8 3 8\n",
      "tgt: 3 5 6 8 8\n",
      "\n",
      "src: 3 3 5 2 4\n",
      "tgt: 2 3 3 4 5\n",
      "\n",
      "src: 6 4 8 2 3\n",
      "tgt: 2 3 4 6 8\n",
      "\n",
      "src: 7 6 9 0 4\n",
      "tgt: 0 4 6 7 9\n",
      "\n",
      "src: 0 7 9 5 6\n",
      "tgt: 0 5 6 7 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "for i in range(-10, 0, 1):\n",
    "    print('src:', raw_val_xs[i])\n",
    "    print('tgt:', raw_val_ys[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white space tokenization\n",
    "val_xs = white_space_tokenizer(raw_val_xs)\n",
    "val_ys = white_space_tokenizer(raw_val_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: 2 7 5 5 6\n",
      "tgt: 2 5 5 6 7\n",
      "\n",
      "src: 6 4 7 3 3\n",
      "tgt: 3 3 4 6 7\n",
      "\n",
      "src: 4 8 3 0 8\n",
      "tgt: 0 3 4 8 8\n",
      "\n",
      "src: 2 2 6 2 0\n",
      "tgt: 0 2 2 2 6\n",
      "\n",
      "src: 5 1 9 9 4\n",
      "tgt: 1 4 5 9 9\n",
      "\n",
      "src: 3 7 6 8 6\n",
      "tgt: 3 6 6 7 8\n",
      "\n",
      "src: 6 3 8 2 2\n",
      "tgt: 2 2 3 6 8\n",
      "\n",
      "src: 1 7 5 7 0\n",
      "tgt: 0 1 5 7 7\n",
      "\n",
      "src: 1 6 7 6 8\n",
      "tgt: 1 6 6 7 8\n",
      "\n",
      "src: 2 7 0 0 0\n",
      "tgt: 0 0 0 2 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "for i in range(-10, 0, 1):\n",
    "    print('src:', raw_test_xs[i])\n",
    "    print('tgt:', raw_test_ys[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white space tokenization\n",
    "test_xs = white_space_tokenizer(raw_test_xs)\n",
    "test_ys = white_space_tokenizer(raw_test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data sets to a dict\n",
    "train_dict = {}\n",
    "train_dict['xs'] = train_xs\n",
    "train_dict['ys'] = train_ys\n",
    "\n",
    "val_dict = {}\n",
    "val_dict['xs'] = val_xs\n",
    "val_dict['ys'] = val_ys\n",
    "\n",
    "test_dict = {}\n",
    "test_dict['xs'] = test_xs\n",
    "test_dict['ys'] = test_ys\n",
    "\n",
    "data_dict = dict()\n",
    "data_dict['train'] = train_dict\n",
    "data_dict['val'] = val_dict\n",
    "data_dict['test'] = test_dict\n",
    "\n",
    "vocab_dict = dict()\n",
    "vocab_dict['src'] = src_vocab2idx_dict\n",
    "vocab_dict['tgt'] = tgt_vocab2idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output as json\n",
    "data_path = os.path.join(outdir, 'data.json')\n",
    "vocab_path = os.path.join(outdir, 'vocab.json')\n",
    "\n",
    "save_json(data_path, data_dict)\n",
    "save_json(vocab_path, vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4],\n",
       "       [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(10).reshape(2, 5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.arange(1, -1, -1).reshape(-1, 1).repeat(5, -1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1],\n",
       "       [5, 5, 5, 5, 5]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.take_along_axis(a, b, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
