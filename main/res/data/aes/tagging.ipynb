{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging Moldes\n",
    "+ Naive GRU RNN\n",
    "+ Naive LSTM RNN\n",
    "+ Bi-directional GRU RNN\n",
    "+ Bi-directional LSTM RNN\n",
    "+ Bi-directional GRU RNN with Attention\n",
    "+ Bi-directional LSTM RNN with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "+ Encoder and Decoder have separate embedding layers\n",
    "+ There are two training methods, namely, online and offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "__author__ = 'Shining'\n",
    "__email__ = 'mrshininnnnn@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency\n",
    "# public\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "# private\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "num_size = 100\n",
    "seq_len = 5\n",
    "data_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aes/num_size_100/seq_len_5/data_size_10000'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load path\n",
    "data_src = 'aes'\n",
    "indir = os.path.join(data_src, \n",
    "                     'num_size_{}'.format(num_size), \n",
    "                     'seq_len_{}'.format(seq_len), \n",
    "                     'data_size_{}'.format(data_size))\n",
    "indir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tagging/num_size_100/seq_len_5/data_size_10000'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save path\n",
    "outdir = 'tagging'\n",
    "\n",
    "outdir = os.path.join(outdir, \n",
    "                      'num_size_{}'.format(num_size), \n",
    "                      'seq_len_{}'.format(seq_len), \n",
    "                      'data_size_{}'.format(data_size))\n",
    "if not os.path.exists(outdir): \n",
    "    os.makedirs(outdir)\n",
    "outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw dataset\n",
    "raw_train_xs = load_txt(os.path.join(indir, 'train_x.txt'))\n",
    "raw_train_ys = load_txt(os.path.join(indir, 'train_y.txt'))\n",
    "raw_val_xs = load_txt(os.path.join(indir, 'val_x.txt'))\n",
    "raw_val_ys = load_txt(os.path.join(indir, 'val_y.txt'))\n",
    "raw_test_xs = load_txt(os.path.join(indir, 'test_x.txt'))\n",
    "raw_test_ys = load_txt(os.path.join(indir, 'test_y.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample size 7000\n",
      "train label size 7000\n",
      "val sample size 1500\n",
      "val label size 1500\n",
      "test sample size 1500\n",
      "test label size 1500\n"
     ]
    }
   ],
   "source": [
    "# check data size\n",
    "print('train sample size', len(raw_train_xs))\n",
    "print('train label size', len(raw_train_ys))\n",
    "print('val sample size', len(raw_val_xs))\n",
    "print('val label size', len(raw_val_ys))\n",
    "print('test sample size', len(raw_test_xs))\n",
    "print('test label size', len(raw_test_ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white space tokenization\n",
    "train_ys = white_space_tokenizer(raw_train_ys)\n",
    "train_xs = white_space_tokenizer(raw_train_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: ['-', '(', '77', '-', '71', ')', '*', '(', '-', '44', '+', '67', ')', '+', '(', '-', '6', '+', '76', ')', '+', '(', '46', '+', '46', ')', '==', '24']\n",
      "tgt: ['-', '6', '*', '23', '+', '70', '+', '92', '==', '24']\n",
      "\n",
      "src: ['-', '(', '5', '+', '89', ')', '+', '36', '+', '(', '34', '+', '19', ')', '+', '(', '89', '+', '7', ')', '==', '91']\n",
      "tgt: ['-', '94', '+', '36', '+', '53', '+', '96', '==', '91']\n",
      "\n",
      "src: ['(', '-', '4', '+', '95', ')', '+', '35', '-', '(', '23', '*', '3', ')', '-', '21', '==', '36']\n",
      "tgt: ['91', '+', '35', '-', '69', '-', '21', '==', '36']\n",
      "\n",
      "src: ['-', '(', '37', '+', '31', ')', '-', '3', '+', '(', '34', '+', '55', ')', '+', '37', '==', '55']\n",
      "tgt: ['-', '68', '-', '3', '+', '89', '+', '37', '==', '55']\n",
      "\n",
      "src: ['-', '23', '-', '(', '-', '25', '+', '48', ')', '-', '34', '+', '93', '==', '13']\n",
      "tgt: ['-', '23', '-', '23', '-', '34', '+', '93', '==', '13']\n",
      "\n",
      "src: ['-', '21', '+', '20', '+', '60', '+', '10', '==', '69']\n",
      "tgt: ['-', '21', '+', '20', '+', '60', '+', '10', '==', '69']\n",
      "\n",
      "src: ['(', '-', '50', '+', '101', ')', '+', '(', '-', '22', '+', '29', ')', '-', '69', '+', '(', '101', '-', '69', ')', '==', '(', '9', '+', '12', ')']\n",
      "tgt: ['51', '+', '7', '-', '69', '+', '32', '==', '21']\n",
      "\n",
      "src: ['-', '50', '-', '50', '+', '62', '+', '55', '==', '17']\n",
      "tgt: ['-', '50', '-', '50', '+', '62', '+', '55', '==', '17']\n",
      "\n",
      "src: ['-', '80', '+', '100', '-', '38', '+', '28', '==', '10']\n",
      "tgt: ['-', '80', '+', '100', '-', '38', '+', '28', '==', '10']\n",
      "\n",
      "src: ['38', '/', '38', '*', '39', '+', '11', '==', '(', '22', '+', '28', ')']\n",
      "tgt: ['38', '/', '38', '*', '39', '+', '11', '==', '50']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "for i in range(-10, 0, 1):\n",
    "    print('src:', train_xs[i])\n",
    "    print('tgt:', train_ys[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "[('+', 20542), ('-', 19642), ('(', 14028), (')', 14028), ('==', 7000), ('*', 1287), ('/', 918), ('2', 797), ('4', 738), ('3', 728), ('7', 696), ('6', 677), ('8', 650), ('10', 643), ('5', 642), ('13', 611), ('12', 602), ('16', 599), ('11', 596), ('23', 595), ('26', 590), ('9', 586), ('15', 582), ('25', 580), ('21', 580), ('22', 580), ('17', 579), ('19', 572), ('31', 565), ('20', 564), ('30', 559), ('29', 558), ('27', 556), ('24', 556), ('14', 553), ('34', 551), ('42', 545), ('36', 532), ('33', 530), ('28', 527), ('32', 526), ('41', 523), ('18', 519), ('40', 513), ('56', 510), ('46', 508), ('44', 503), ('39', 502), ('50', 499), ('52', 499), ('38', 494), ('60', 493), ('59', 488), ('58', 485), ('43', 484), ('35', 482), ('57', 480), ('49', 477), ('37', 477), ('51', 476), ('48', 476), ('63', 474), ('69', 465), ('54', 463), ('45', 461), ('65', 459), ('70', 458), ('47', 457), ('61', 456), ('64', 454), ('53', 453), ('55', 442), ('77', 440), ('66', 437), ('75', 437), ('72', 435), ('76', 428), ('68', 428), ('67', 425), ('89', 424), ('74', 423), ('73', 422), ('81', 422), ('62', 420), ('84', 411), ('79', 407), ('90', 404), ('78', 399), ('82', 397), ('71', 396), ('92', 390), ('80', 389), ('83', 386), ('93', 382), ('91', 380), ('86', 374), ('88', 371), ('85', 363), ('100', 362), ('87', 362), ('96', 362), ('94', 361), ('97', 359), ('98', 353), ('99', 353), ('95', 334), ('101', 317)]\n"
     ]
    }
   ],
   "source": [
    "# source vocabulary frequency distribution\n",
    "counter = Counter()\n",
    "for x in train_xs:\n",
    "    counter.update(x)\n",
    "\n",
    "print(len(counter))\n",
    "print(counter.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', ')', '*', '+', '-', '/', '10', '100', '101', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '==']\n"
     ]
    }
   ],
   "source": [
    "src_vocab_list = sorted(counter.keys())\n",
    "print(src_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '(': 1, ')': 2, '*': 3, '+': 4, '-': 5, '/': 6, '10': 7, '100': 8, '101': 9, '11': 10, '12': 11, '13': 12, '14': 13, '15': 14, '16': 15, '17': 16, '18': 17, '19': 18, '2': 19, '20': 20, '21': 21, '22': 22, '23': 23, '24': 24, '25': 25, '26': 26, '27': 27, '28': 28, '29': 29, '3': 30, '30': 31, '31': 32, '32': 33, '33': 34, '34': 35, '35': 36, '36': 37, '37': 38, '38': 39, '39': 40, '4': 41, '40': 42, '41': 43, '42': 44, '43': 45, '44': 46, '45': 47, '46': 48, '47': 49, '48': 50, '49': 51, '5': 52, '50': 53, '51': 54, '52': 55, '53': 56, '54': 57, '55': 58, '56': 59, '57': 60, '58': 61, '59': 62, '6': 63, '60': 64, '61': 65, '62': 66, '63': 67, '64': 68, '65': 69, '66': 70, '67': 71, '68': 72, '69': 73, '7': 74, '70': 75, '71': 76, '72': 77, '73': 78, '74': 79, '75': 80, '76': 81, '77': 82, '78': 83, '79': 84, '8': 85, '80': 86, '81': 87, '82': 88, '83': 89, '84': 90, '85': 91, '86': 92, '87': 93, '88': 94, '89': 95, '9': 96, '90': 97, '91': 98, '92': 99, '93': 100, '94': 101, '95': 102, '96': 103, '97': 104, '98': 105, '99': 106, '==': 107}\n"
     ]
    }
   ],
   "source": [
    "# soruce vocabulary dictionary\n",
    "src_vocab2idx_dict = dict()\n",
    "src_vocab2idx_dict['<pad>'] = 0 # to pad sequence length\n",
    "\n",
    "i = len(src_vocab2idx_dict)\n",
    "for token in src_vocab_list:\n",
    "    src_vocab2idx_dict[token] = i\n",
    "    i += 1\n",
    "\n",
    "print(src_vocab2idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "['<delete>', '<keep>', '<add_2>', '<add_3>', '<add_4>', '<add_5>', '<add_6>', '<add_7>', '<add_8>', '<add_9>', '<add_10>', '<add_11>', '<add_12>', '<add_13>', '<add_14>', '<add_15>', '<add_16>', '<add_17>', '<add_18>', '<add_19>', '<add_20>', '<add_21>', '<add_22>', '<add_23>', '<add_24>', '<add_25>', '<add_26>', '<add_27>', '<add_28>', '<add_29>', '<add_30>', '<add_31>', '<add_32>', '<add_33>', '<add_34>', '<add_35>', '<add_36>', '<add_37>', '<add_38>', '<add_39>', '<add_40>', '<add_41>', '<add_42>', '<add_43>', '<add_44>', '<add_45>', '<add_46>', '<add_47>', '<add_48>', '<add_49>', '<add_50>', '<add_51>', '<add_52>', '<add_53>', '<add_54>', '<add_55>', '<add_56>', '<add_57>', '<add_58>', '<add_59>', '<add_60>', '<add_61>', '<add_62>', '<add_63>', '<add_64>', '<add_65>', '<add_66>', '<add_67>', '<add_68>', '<add_69>', '<add_70>', '<add_71>', '<add_72>', '<add_73>', '<add_74>', '<add_75>', '<add_76>', '<add_77>', '<add_78>', '<add_79>', '<add_80>', '<add_81>', '<add_82>', '<add_83>', '<add_84>', '<add_85>', '<add_86>', '<add_87>', '<add_88>', '<add_89>', '<add_90>', '<add_91>', '<add_92>', '<add_93>', '<add_94>', '<add_95>', '<add_96>', '<add_97>', '<add_98>', '<add_99>', '<add_100>', '<add_101>']\n"
     ]
    }
   ],
   "source": [
    "tgt_vocab_list = ['<delete>', '<keep>']\n",
    "tgt_vocab_list += ['<add_{}>'.format(i) for i in range(2, num_size+2)]\n",
    "print(len(tgt_vocab_list))\n",
    "print(tgt_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<s>': 1, '</s>': 2, '<delete>': 3, '<keep>': 4, '<add_2>': 5, '<add_3>': 6, '<add_4>': 7, '<add_5>': 8, '<add_6>': 9, '<add_7>': 10, '<add_8>': 11, '<add_9>': 12, '<add_10>': 13, '<add_11>': 14, '<add_12>': 15, '<add_13>': 16, '<add_14>': 17, '<add_15>': 18, '<add_16>': 19, '<add_17>': 20, '<add_18>': 21, '<add_19>': 22, '<add_20>': 23, '<add_21>': 24, '<add_22>': 25, '<add_23>': 26, '<add_24>': 27, '<add_25>': 28, '<add_26>': 29, '<add_27>': 30, '<add_28>': 31, '<add_29>': 32, '<add_30>': 33, '<add_31>': 34, '<add_32>': 35, '<add_33>': 36, '<add_34>': 37, '<add_35>': 38, '<add_36>': 39, '<add_37>': 40, '<add_38>': 41, '<add_39>': 42, '<add_40>': 43, '<add_41>': 44, '<add_42>': 45, '<add_43>': 46, '<add_44>': 47, '<add_45>': 48, '<add_46>': 49, '<add_47>': 50, '<add_48>': 51, '<add_49>': 52, '<add_50>': 53, '<add_51>': 54, '<add_52>': 55, '<add_53>': 56, '<add_54>': 57, '<add_55>': 58, '<add_56>': 59, '<add_57>': 60, '<add_58>': 61, '<add_59>': 62, '<add_60>': 63, '<add_61>': 64, '<add_62>': 65, '<add_63>': 66, '<add_64>': 67, '<add_65>': 68, '<add_66>': 69, '<add_67>': 70, '<add_68>': 71, '<add_69>': 72, '<add_70>': 73, '<add_71>': 74, '<add_72>': 75, '<add_73>': 76, '<add_74>': 77, '<add_75>': 78, '<add_76>': 79, '<add_77>': 80, '<add_78>': 81, '<add_79>': 82, '<add_80>': 83, '<add_81>': 84, '<add_82>': 85, '<add_83>': 86, '<add_84>': 87, '<add_85>': 88, '<add_86>': 89, '<add_87>': 90, '<add_88>': 91, '<add_89>': 92, '<add_90>': 93, '<add_91>': 94, '<add_92>': 95, '<add_93>': 96, '<add_94>': 97, '<add_95>': 98, '<add_96>': 99, '<add_97>': 100, '<add_98>': 101, '<add_99>': 102, '<add_100>': 103, '<add_101>': 104}\n"
     ]
    }
   ],
   "source": [
    "# target vocabulary dictionary\n",
    "tgt_vocab2idx_dict = dict()\n",
    "tgt_vocab2idx_dict['<pad>'] = 0 # to pad sequence length\n",
    "tgt_vocab2idx_dict['<s>'] = 1 # to mark the start of a sequence\n",
    "tgt_vocab2idx_dict['</s>'] = 2 # to mark the end of a sequence\n",
    "\n",
    "i = len(tgt_vocab2idx_dict)\n",
    "for token in tgt_vocab_list:\n",
    "    tgt_vocab2idx_dict[token] = i\n",
    "    i += 1\n",
    "\n",
    "print(tgt_vocab2idx_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white space tokenization\n",
    "val_xs = white_space_tokenizer(raw_val_xs)\n",
    "val_ys = white_space_tokenizer(raw_val_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: ['-', '(', '4', '+', '52', ')', '/', '(', '-', '26', '+', '34', ')', '+', '46', '+', '(', '-', '8', '+', '62', ')', '==', '(', '91', '+', '2', ')']\n",
      "tgt: ['-', '56', '/', '8', '+', '46', '+', '54', '==', '93']\n",
      "\n",
      "src: ['-', '(', '84', '+', '6', ')', '/', '(', '92', '-', '87', ')', '+', '88', '+', '30', '==', '100']\n",
      "tgt: ['-', '90', '/', '5', '+', '88', '+', '30', '==', '100']\n",
      "\n",
      "src: ['9', '+', '(', '-', '4', '+', '10', ')', '+', '21', '+', '2', '==', '38']\n",
      "tgt: ['9', '+', '6', '+', '21', '+', '2', '==', '38']\n",
      "\n",
      "src: ['-', '33', '+', '94', '-', '76', '+', '23', '==', '8']\n",
      "tgt: ['-', '33', '+', '94', '-', '76', '+', '23', '==', '8']\n",
      "\n",
      "src: ['51', '-', '(', '-', '50', '+', '66', ')', '*', '(', '37', '-', '34', ')', '+', '(', '78', '-', '7', ')', '==', '74']\n",
      "tgt: ['51', '-', '16', '*', '3', '+', '71', '==', '74']\n",
      "\n",
      "src: ['-', '85', '+', '71', '+', '88', '+', '25', '==', '99']\n",
      "tgt: ['-', '85', '+', '71', '+', '88', '+', '25', '==', '99']\n",
      "\n",
      "src: ['-', '79', '-', '22', '+', '(', '-', '4', '+', '84', ')', '+', '91', '==', '70']\n",
      "tgt: ['-', '79', '-', '22', '+', '80', '+', '91', '==', '70']\n",
      "\n",
      "src: ['5', '+', '59', '-', '(', '-', '94', '+', '98', ')', '-', '(', '-', '38', '+', '62', ')', '==', '(', '-', '12', '+', '48', ')']\n",
      "tgt: ['5', '+', '59', '-', '4', '-', '24', '==', '36']\n",
      "\n",
      "src: ['(', '-', '17', '+', '48', ')', '-', '(', '30', '-', '11', ')', '+', '58', '-', '(', '99', '-', '86', ')', '==', '(', '51', '+', '6', ')']\n",
      "tgt: ['31', '-', '19', '+', '58', '-', '13', '==', '57']\n",
      "\n",
      "src: ['(', '-', '3', '+', '15', ')', '+', '15', '+', '70', '-', '46', '==', '51']\n",
      "tgt: ['12', '+', '15', '+', '70', '-', '46', '==', '51']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "for i in range(-10, 0, 1):\n",
    "    print('src:', val_xs[i])\n",
    "    print('tgt:', val_ys[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white space tokenization\n",
    "test_xs = white_space_tokenizer(raw_test_xs)\n",
    "test_ys = white_space_tokenizer(raw_test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: ['-', '18', '+', '(', '-', '41', '+', '85', ')', '+', '(', '38', '+', '3', ')', '-', '(', '19', '+', '33', ')', '==', '(', '-', '78', '+', '93', ')']\n",
      "tgt: ['-', '18', '+', '44', '+', '41', '-', '52', '==', '15']\n",
      "\n",
      "src: ['-', '95', '+', '(', '75', '+', '21', ')', '+', '(', '10', '+', '79', ')', '-', '12', '==', '78']\n",
      "tgt: ['-', '95', '+', '96', '+', '89', '-', '12', '==', '78']\n",
      "\n",
      "src: ['37', '+', '2', '+', '(', '50', '+', '33', ')', '-', '(', '44', '+', '9', ')', '==', '(', '72', '-', '3', ')']\n",
      "tgt: ['37', '+', '2', '+', '83', '-', '53', '==', '69']\n",
      "\n",
      "src: ['(', '-', '34', '+', '76', ')', '+', '(', '76', '+', '13', ')', '-', '(', '100', '-', '4', ')', '+', '36', '==', '71']\n",
      "tgt: ['42', '+', '89', '-', '96', '+', '36', '==', '71']\n",
      "\n",
      "src: ['88', '-', '2', '+', '66', '-', '81', '==', '(', '85', '-', '14', ')']\n",
      "tgt: ['88', '-', '2', '+', '66', '-', '81', '==', '71']\n",
      "\n",
      "src: ['-', '38', '-', '32', '+', '7', '*', '17', '==', '49']\n",
      "tgt: ['-', '38', '-', '32', '+', '7', '*', '17', '==', '49']\n",
      "\n",
      "src: ['(', '-', '27', '+', '44', ')', '+', '(', '12', '+', '70', ')', '-', '12', '-', '(', '-', '3', '+', '60', ')', '==', '30']\n",
      "tgt: ['17', '+', '82', '-', '12', '-', '57', '==', '30']\n",
      "\n",
      "src: ['-', '6', '-', '50', '+', '38', '+', '(', '18', '+', '49', ')', '==', '49']\n",
      "tgt: ['-', '6', '-', '50', '+', '38', '+', '67', '==', '49']\n",
      "\n",
      "src: ['47', '-', '44', '+', '98', '-', '46', '==', '55']\n",
      "tgt: ['47', '-', '44', '+', '98', '-', '46', '==', '55']\n",
      "\n",
      "src: ['95', '+', '60', '-', '(', '100', '-', '29', ')', '-', '70', '==', '14']\n",
      "tgt: ['95', '+', '60', '-', '71', '-', '70', '==', '14']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "for i in range(-10, 0, 1):\n",
    "    print('src:', test_xs[i])\n",
    "    print('tgt:', test_ys[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data sets to a dict\n",
    "train_dict = {}\n",
    "train_dict['ys'] = train_ys\n",
    "\n",
    "val_dict = {}\n",
    "val_dict['xs'] = val_xs\n",
    "val_dict['ys'] = val_ys\n",
    "\n",
    "test_dict = {}\n",
    "test_dict['xs'] = test_xs\n",
    "test_dict['ys'] = test_ys\n",
    "\n",
    "data_dict = dict()\n",
    "data_dict['train'] = train_dict\n",
    "data_dict['val'] = val_dict\n",
    "data_dict['test'] = test_dict\n",
    "\n",
    "vocab_dict = dict()\n",
    "vocab_dict['src'] = src_vocab2idx_dict\n",
    "vocab_dict['tgt'] = tgt_vocab2idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output as json\n",
    "data_path = os.path.join(outdir, 'data.json')\n",
    "vocab_path = os.path.join(outdir, 'vocab.json')\n",
    "\n",
    "save_json(data_path, data_dict)\n",
    "save_json(vocab_path, vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_tag_pair(x, y):\n",
    "#     x_ = x.split()\n",
    "#     y = y.split()\n",
    "#     y_ = []\n",
    "#     x_token = x_.pop(0)\n",
    "#     for i in range(len(y)):\n",
    "#         y_token = y[i]\n",
    "#         if x_token == y_token:\n",
    "#             y_.append('<keep>')\n",
    "#             if len(x_) == 0:\n",
    "#                 break\n",
    "#             x_token = x_.pop(0)\n",
    "#         else:\n",
    "#             y_.append('<add_{}>'.format(y_token))\n",
    "#             while True:\n",
    "#                 y_.append('<delete>')\n",
    "#                 if x_token == ')':\n",
    "#                     if len(x_) != 0:\n",
    "#                         x_token = x_.pop(0)\n",
    "#                     break\n",
    "#                 x_token = x_.pop(0)\n",
    "#     return x, ' '.join(y), ' '.join(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_xs, train_ys, train_ys_ = zip(*[gen_tag_pair(x, y) for x, y in zip(raw_train_xs, raw_train_ys)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
