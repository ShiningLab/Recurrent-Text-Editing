{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End2End Moldes\n",
    "+ Naive RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "+ Both the encoder and decoder share the same embedding layer\n",
    "+ Both the train_valid and test share the same vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "__author__ = 'Shining'\n",
    "__email__ = 'mrshininnnnn@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from utils import load_txt, save_json, white_space_tokenizer, vocab_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "vocab_size = 10\n",
    "seq_len = 10\n",
    "data_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raw/vocab_size_10/seq_len_10/data_size_100000'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load path\n",
    "indir = 'raw'\n",
    "indir = os.path.join(indir, 'vocab_size_{}'.format(vocab_size), \n",
    "                     'seq_len_{}'.format(seq_len), \n",
    "                     'data_size_{}'.format(data_size))\n",
    "indir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end2end/vocab_size_10/seq_len_10/data_size_100000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save path\n",
    "outdir = 'end2end'\n",
    "\n",
    "outdir = os.path.join(outdir, 'vocab_size_{}'.format(vocab_size), \n",
    "                      'seq_len_{}'.format(seq_len), \n",
    "                      'data_size_{}'.format(data_size))\n",
    "if not os.path.exists(outdir): \n",
    "    os.makedirs(outdir)\n",
    "outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw dataset\n",
    "raw_xs = load_txt(os.path.join(indir, 'x.txt'))\n",
    "raw_ys = load_txt(os.path.join(indir, 'y.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size 100000\n",
      "label size 100000\n"
     ]
    }
   ],
   "source": [
    "# check data size\n",
    "print('sample size', len(raw_xs))\n",
    "print('label size', len(raw_ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n"
     ]
    }
   ],
   "source": [
    "# check duplicates\n",
    "dataset = [(src, tgt) for src, tgt in zip(raw_xs, raw_ys)]\n",
    "dataset = np.array(list(set(dataset)))\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 1 8 5 3 3 1 3 9 6 7\n",
      "output: 1 + 8 - 5 / 3 * 3 - 1 / 3 * 9 + 6 == 7\n",
      "\n",
      "input: 4 6 6 9 3 1 4 0 6 3\n",
      "output: 4 - 6 / 6 - 9 * 3 * 1 * 4 * 0 * 6 == 3\n",
      "\n",
      "input: 4 1 6 0 8 0 1 4 8 4\n",
      "output: 4 / 1 / 6 * 0 / 8 + 0 * 1 - 4 + 8 == 4\n",
      "\n",
      "input: 4 9 0 9 2 2 3 9 3 8\n",
      "output: 4 + 9 + 0 - 9 + 2 + 2 - 3 + 9 / 3 == 8\n",
      "\n",
      "input: 5 6 3 9 3 7 0 8 0 4\n",
      "output: 5 + 6 - 3 + 9 / 3 - 7 - 0 * 8 * 0 == 4\n",
      "\n",
      "input: 9 0 4 1 4 2 6 9 2 7\n",
      "output: 9 + 0 + 4 + 1 * 4 / 2 * 6 - 9 * 2 == 7\n",
      "\n",
      "input: 1 4 6 3 4 1 6 3 1 0\n",
      "output: 1 * 4 * 6 * 3 - 4 / 1 * 6 * 3 * 1 == 0\n",
      "\n",
      "input: 2 7 0 5 6 2 0 3 1 4\n",
      "output: 2 + 7 - 0 * 5 - 6 / 2 + 0 - 3 + 1 == 4\n",
      "\n",
      "input: 3 9 9 1 4 3 4 4 5 8\n",
      "output: 3 / 9 * 9 - 1 + 4 - 3 - 4 + 4 + 5 == 8\n",
      "\n",
      "input: 3 3 5 8 7 4 8 0 7 4\n",
      "output: 3 * 3 - 5 - 8 / 7 / 4 * 8 * 0 * 7 == 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "for i in range(-10, 0, 1):\n",
    "    print('input:', dataset[i, 0])\n",
    "    print('output:', dataset[i, 1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white space tokenization\n",
    "xs = dataset[:, 0]\n",
    "ys = dataset[:, 1]\n",
    "tk_xs = white_space_tokenizer(xs)\n",
    "tk_ys = white_space_tokenizer(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', 295062), ('1', 230154), ('*', 229541), ('2', 213860), ('+', 208073), ('-', 203841), ('3', 201328), ('4', 194710), ('6', 184504), ('5', 176640), ('8', 173162), ('7', 166410), ('9', 164170), ('/', 158545), ('==', 100000)]\n"
     ]
    }
   ],
   "source": [
    "# vocabulary frequency distribution\n",
    "counter = Counter()\n",
    "for x in tk_xs:\n",
    "    counter.update(x)\n",
    "\n",
    "for y in tk_ys:\n",
    "    counter.update(y)\n",
    "\n",
    "print(counter.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*', '+', '-', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '==']\n"
     ]
    }
   ],
   "source": [
    "vocab_list = sorted(counter.keys())\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, '*': 4, '+': 5, '-': 6, '/': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '5': 13, '6': 14, '7': 15, '8': 16, '9': 17, '==': 18}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary dictionary\n",
    "vocab2idx_dict = dict()\n",
    "vocab2idx_dict['<pad>'] = 0 # to pad sequence length\n",
    "vocab2idx_dict['<s>'] = 1 # to mark the start of a sequence\n",
    "vocab2idx_dict['</s>'] = 2 # to mark the end of a sequence\n",
    "\n",
    "i = len(vocab2idx_dict)\n",
    "for token in vocab_list:\n",
    "    vocab2idx_dict[token] = i\n",
    "    i += 1\n",
    "\n",
    "print(vocab2idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert vocabulary to index\n",
    "xs = vocab_to_index(tk_xs, vocab2idx_dict)\n",
    "ys = vocab_to_index(tk_ys, vocab2idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [9, 16, 13, 11, 11, 9, 11, 17, 14, 15] ['1', '8', '5', '3', '3', '1', '3', '9', '6', '7']\n",
      "output: [9, 5, 16, 6, 13, 7, 11, 4, 11, 6, 9, 7, 11, 4, 17, 5, 14, 18, 15] ['1', '+', '8', '-', '5', '/', '3', '*', '3', '-', '1', '/', '3', '*', '9', '+', '6', '==', '7']\n",
      "\n",
      "input: [12, 14, 14, 17, 11, 9, 12, 8, 14, 11] ['4', '6', '6', '9', '3', '1', '4', '0', '6', '3']\n",
      "output: [12, 6, 14, 7, 14, 6, 17, 4, 11, 4, 9, 4, 12, 4, 8, 4, 14, 18, 11] ['4', '-', '6', '/', '6', '-', '9', '*', '3', '*', '1', '*', '4', '*', '0', '*', '6', '==', '3']\n",
      "\n",
      "input: [12, 9, 14, 8, 16, 8, 9, 12, 16, 12] ['4', '1', '6', '0', '8', '0', '1', '4', '8', '4']\n",
      "output: [12, 7, 9, 7, 14, 4, 8, 7, 16, 5, 8, 4, 9, 6, 12, 5, 16, 18, 12] ['4', '/', '1', '/', '6', '*', '0', '/', '8', '+', '0', '*', '1', '-', '4', '+', '8', '==', '4']\n",
      "\n",
      "input: [12, 17, 8, 17, 10, 10, 11, 17, 11, 16] ['4', '9', '0', '9', '2', '2', '3', '9', '3', '8']\n",
      "output: [12, 5, 17, 5, 8, 6, 17, 5, 10, 5, 10, 6, 11, 5, 17, 7, 11, 18, 16] ['4', '+', '9', '+', '0', '-', '9', '+', '2', '+', '2', '-', '3', '+', '9', '/', '3', '==', '8']\n",
      "\n",
      "input: [13, 14, 11, 17, 11, 15, 8, 16, 8, 12] ['5', '6', '3', '9', '3', '7', '0', '8', '0', '4']\n",
      "output: [13, 5, 14, 6, 11, 5, 17, 7, 11, 6, 15, 6, 8, 4, 16, 4, 8, 18, 12] ['5', '+', '6', '-', '3', '+', '9', '/', '3', '-', '7', '-', '0', '*', '8', '*', '0', '==', '4']\n",
      "\n",
      "input: [17, 8, 12, 9, 12, 10, 14, 17, 10, 15] ['9', '0', '4', '1', '4', '2', '6', '9', '2', '7']\n",
      "output: [17, 5, 8, 5, 12, 5, 9, 4, 12, 7, 10, 4, 14, 6, 17, 4, 10, 18, 15] ['9', '+', '0', '+', '4', '+', '1', '*', '4', '/', '2', '*', '6', '-', '9', '*', '2', '==', '7']\n",
      "\n",
      "input: [9, 12, 14, 11, 12, 9, 14, 11, 9, 8] ['1', '4', '6', '3', '4', '1', '6', '3', '1', '0']\n",
      "output: [9, 4, 12, 4, 14, 4, 11, 6, 12, 7, 9, 4, 14, 4, 11, 4, 9, 18, 8] ['1', '*', '4', '*', '6', '*', '3', '-', '4', '/', '1', '*', '6', '*', '3', '*', '1', '==', '0']\n",
      "\n",
      "input: [10, 15, 8, 13, 14, 10, 8, 11, 9, 12] ['2', '7', '0', '5', '6', '2', '0', '3', '1', '4']\n",
      "output: [10, 5, 15, 6, 8, 4, 13, 6, 14, 7, 10, 5, 8, 6, 11, 5, 9, 18, 12] ['2', '+', '7', '-', '0', '*', '5', '-', '6', '/', '2', '+', '0', '-', '3', '+', '1', '==', '4']\n",
      "\n",
      "input: [11, 17, 17, 9, 12, 11, 12, 12, 13, 16] ['3', '9', '9', '1', '4', '3', '4', '4', '5', '8']\n",
      "output: [11, 7, 17, 4, 17, 6, 9, 5, 12, 6, 11, 6, 12, 5, 12, 5, 13, 18, 16] ['3', '/', '9', '*', '9', '-', '1', '+', '4', '-', '3', '-', '4', '+', '4', '+', '5', '==', '8']\n",
      "\n",
      "input: [11, 11, 13, 16, 15, 12, 16, 8, 15, 12] ['3', '3', '5', '8', '7', '4', '8', '0', '7', '4']\n",
      "output: [11, 4, 11, 6, 13, 6, 16, 7, 15, 7, 12, 4, 16, 4, 8, 4, 15, 18, 12] ['3', '*', '3', '-', '5', '-', '8', '/', '7', '/', '4', '*', '8', '*', '0', '*', '7', '==', '4']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "for i in range(-10, 0, 1):\n",
    "    print('input:', xs[i], tk_xs[i])\n",
    "    print('output:', ys[i], tk_ys[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 80000 80000\n",
      "test size 20000 20000\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "dataset = np.array([(x, y) for x, y in zip(xs, ys)])\n",
    "data_size = dataset.shape[0]\n",
    "indices = np.random.permutation(data_size)\n",
    "train_size = int(0.8*data_size)\n",
    "test_size = int(0.2*data_size)\n",
    "train_idx = indices[:train_size]\n",
    "test_idx = indices[train_size:]\n",
    "train_set = dataset[train_idx, :]\n",
    "test_set = dataset[test_idx, :]\n",
    "print('train size', train_size, train_set.shape[0])\n",
    "print('test size', test_size, test_set.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data sets to a dict\n",
    "train_dict = {}\n",
    "train_dict['xs'] = train_set[:, 0].tolist()\n",
    "train_dict['ys'] = train_set[:, 1].tolist()\n",
    "\n",
    "test_dict = {}\n",
    "test_dict['xs'] = test_set[:, 0].tolist()\n",
    "test_dict['ys'] = test_set[:, 1].tolist()\n",
    "\n",
    "data_dict = dict()\n",
    "data_dict['train'] = train_dict\n",
    "data_dict['test'] = test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output as json\n",
    "data_path = os.path.join(outdir, 'data.json')\n",
    "vocab_path = os.path.join(outdir, 'vocab.json')\n",
    "\n",
    "save_json(data_path, data_dict)\n",
    "save_json(vocab_path, vocab2idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
