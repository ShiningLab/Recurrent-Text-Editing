{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End2End Moldes\n",
    "+ Naive RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "+ Both the encoder and decoder share the same embedding layer\n",
    "+ Both the train_valid and test share the same vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "__author__ = 'Shining'\n",
    "__email__ = 'mrshininnnnn@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from utils import load_txt, save_json, white_space_tokenizer, vocab_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "vocab_size = 10\n",
    "seq_len = 5\n",
    "data_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raw/vocab_size_10/seq_len_5/data_size_10000'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load path\n",
    "indir = 'raw'\n",
    "indir = os.path.join(indir, 'vocab_size_{}'.format(vocab_size), \n",
    "                     'seq_len_{}'.format(seq_len), \n",
    "                     'data_size_{}'.format(data_size))\n",
    "indir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end2end/vocab_size_10/seq_len_5/data_size_10000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save path\n",
    "outdir = 'end2end'\n",
    "\n",
    "outdir = os.path.join(outdir, 'vocab_size_{}'.format(vocab_size), \n",
    "                      'seq_len_{}'.format(seq_len), \n",
    "                      'data_size_{}'.format(data_size))\n",
    "if not os.path.exists(outdir): \n",
    "    os.makedirs(outdir)\n",
    "outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw dataset\n",
    "raw_xs = load_txt(os.path.join(indir, 'x.txt'))\n",
    "raw_ys = load_txt(os.path.join(indir, 'y.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size 10000\n",
      "label size 10000\n"
     ]
    }
   ],
   "source": [
    "# check data size\n",
    "print('sample size', len(raw_xs))\n",
    "print('label size', len(raw_ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "# check duplicates\n",
    "dataset = [(src, tgt) for src, tgt in zip(raw_xs, raw_ys)]\n",
    "dataset = np.array(list(set(dataset)))\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 8 1 3 0 5\n",
      "output: 8 / 1 - 3 - 0 == 5\n",
      "\n",
      "input: 2 7 2 6 2\n",
      "output: 2 * 7 - 2 * 6 == 2\n",
      "\n",
      "input: 7 9 1 9 6\n",
      "output: 7 - 9 - 1 + 9 == 6\n",
      "\n",
      "input: 7 9 6 8 2\n",
      "output: 7 + 9 - 6 - 8 == 2\n",
      "\n",
      "input: 0 8 1 6 7\n",
      "output: 0 / 8 + 1 + 6 == 7\n",
      "\n",
      "input: 4 7 7 0 0\n",
      "output: 4 / 7 * 7 * 0 == 0\n",
      "\n",
      "input: 7 1 1 1 7\n",
      "output: 7 / 1 * 1 * 1 == 7\n",
      "\n",
      "input: 0 7 6 1 1\n",
      "output: 0 * 7 / 6 + 1 == 1\n",
      "\n",
      "input: 8 1 0 9 0\n",
      "output: 8 + 1 - 0 - 9 == 0\n",
      "\n",
      "input: 0 5 1 1 1\n",
      "output: 0 * 5 / 1 + 1 == 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "for i in range(-10, 0, 1):\n",
    "    print('input:', dataset[i, 0])\n",
    "    print('output:', dataset[i, 1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white space tokenization\n",
    "dataset = np.array(dataset)\n",
    "xs = dataset[:, 0]\n",
    "ys = dataset[:, 1]\n",
    "tk_xs = white_space_tokenizer(xs)\n",
    "tk_ys = white_space_tokenizer(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', 17688), ('1', 11854), ('2', 10686), ('==', 10000), ('3', 9684), ('4', 9552), ('6', 8860), ('+', 8290), ('5', 8270), ('*', 8057), ('7', 7994), ('8', 7866), ('-', 7808), ('9', 7546), ('/', 5845)]\n"
     ]
    }
   ],
   "source": [
    "# vocabulary frequency distribution\n",
    "counter = Counter()\n",
    "for x in tk_xs:\n",
    "    counter.update(x)\n",
    "\n",
    "for y in tk_ys:\n",
    "    counter.update(y)\n",
    "\n",
    "print(counter.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*', '+', '-', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '==']\n"
     ]
    }
   ],
   "source": [
    "vocab_list = sorted(counter.keys())\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, '*': 4, '+': 5, '-': 6, '/': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '5': 13, '6': 14, '7': 15, '8': 16, '9': 17, '==': 18}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary dictionary\n",
    "vocab2idx_dict = dict()\n",
    "vocab2idx_dict['<pad>'] = 0 # to pad sequence length\n",
    "vocab2idx_dict['<s>'] = 1 # to mark the start of a sequence\n",
    "vocab2idx_dict['</s>'] = 2 # to mark the end of a sequence\n",
    "vocab2idx_dict['<unk>'] = 3 # to represent the unknow word\n",
    "\n",
    "i = len(vocab2idx_dict)\n",
    "for token in vocab_list:\n",
    "    vocab2idx_dict[token] = i\n",
    "    i += 1\n",
    "\n",
    "print(vocab2idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert vocabulary to index\n",
    "xs = vocab_to_index(tk_xs, vocab2idx_dict)\n",
    "ys = vocab_to_index(tk_ys, vocab2idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [16, 9, 11, 8, 13] ['8', '1', '3', '0', '5']\n",
      "output: [16, 7, 9, 6, 11, 6, 8, 18, 13] ['8', '/', '1', '-', '3', '-', '0', '==', '5']\n",
      "\n",
      "input: [10, 15, 10, 14, 10] ['2', '7', '2', '6', '2']\n",
      "output: [10, 4, 15, 6, 10, 4, 14, 18, 10] ['2', '*', '7', '-', '2', '*', '6', '==', '2']\n",
      "\n",
      "input: [15, 17, 9, 17, 14] ['7', '9', '1', '9', '6']\n",
      "output: [15, 6, 17, 6, 9, 5, 17, 18, 14] ['7', '-', '9', '-', '1', '+', '9', '==', '6']\n",
      "\n",
      "input: [15, 17, 14, 16, 10] ['7', '9', '6', '8', '2']\n",
      "output: [15, 5, 17, 6, 14, 6, 16, 18, 10] ['7', '+', '9', '-', '6', '-', '8', '==', '2']\n",
      "\n",
      "input: [8, 16, 9, 14, 15] ['0', '8', '1', '6', '7']\n",
      "output: [8, 7, 16, 5, 9, 5, 14, 18, 15] ['0', '/', '8', '+', '1', '+', '6', '==', '7']\n",
      "\n",
      "input: [12, 15, 15, 8, 8] ['4', '7', '7', '0', '0']\n",
      "output: [12, 7, 15, 4, 15, 4, 8, 18, 8] ['4', '/', '7', '*', '7', '*', '0', '==', '0']\n",
      "\n",
      "input: [15, 9, 9, 9, 15] ['7', '1', '1', '1', '7']\n",
      "output: [15, 7, 9, 4, 9, 4, 9, 18, 15] ['7', '/', '1', '*', '1', '*', '1', '==', '7']\n",
      "\n",
      "input: [8, 15, 14, 9, 9] ['0', '7', '6', '1', '1']\n",
      "output: [8, 4, 15, 7, 14, 5, 9, 18, 9] ['0', '*', '7', '/', '6', '+', '1', '==', '1']\n",
      "\n",
      "input: [16, 9, 8, 17, 8] ['8', '1', '0', '9', '0']\n",
      "output: [16, 5, 9, 6, 8, 6, 17, 18, 8] ['8', '+', '1', '-', '0', '-', '9', '==', '0']\n",
      "\n",
      "input: [8, 13, 9, 9, 9] ['0', '5', '1', '1', '1']\n",
      "output: [8, 4, 13, 7, 9, 5, 9, 18, 9] ['0', '*', '5', '/', '1', '+', '1', '==', '1']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "for i in range(-10, 0, 1):\n",
    "    print('input:', xs[i], tk_xs[i])\n",
    "    print('output:', ys[i], tk_ys[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 7000 7000\n",
      "valid size 1500 1500\n",
      "test size 1500 1500\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "dataset = np.array([(x, y) for x, y in zip(xs, ys)])\n",
    "data_size = dataset.shape[0]\n",
    "indices = np.random.permutation(data_size)\n",
    "train_size = int(0.7*data_size)\n",
    "valid_size = int(0.15*data_size)\n",
    "test_size = int(0.15*data_size)\n",
    "train_idx = indices[:train_size]\n",
    "valid_idx = indices[train_size:train_size+valid_size]\n",
    "test_idx = indices[train_size+valid_size:]\n",
    "train_set = dataset[train_idx, :]\n",
    "valid_set = dataset[valid_idx, :]\n",
    "test_set = dataset[test_idx, :]\n",
    "print('train size', train_size, train_set.shape[0])\n",
    "print('valid size', valid_size, valid_set.shape[0])\n",
    "print('test size', test_size, test_set.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data sets to a dict\n",
    "train_dict = {}\n",
    "train_dict['xs'] = train_set[:, 0].tolist()\n",
    "train_dict['ys'] = train_set[:, 1].tolist()\n",
    "\n",
    "valid_dict = {}\n",
    "valid_dict['xs'] = valid_set[:, 0].tolist()\n",
    "valid_dict['ys'] = valid_set[:, 1].tolist()\n",
    "\n",
    "test_dict = {}\n",
    "test_dict['xs'] = test_set[:, 0].tolist()\n",
    "test_dict['ys'] = test_set[:, 1].tolist()\n",
    "\n",
    "\n",
    "data_dict = dict()\n",
    "data_dict['train'] = train_dict\n",
    "data_dict['valid'] = valid_dict\n",
    "data_dict['test'] = test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output as json\n",
    "data_path = os.path.join(outdir, 'data.json')\n",
    "vocab_path = os.path.join(outdir, 'vocab.json')\n",
    "\n",
    "save_json(data_path, data_dict)\n",
    "save_json(vocab_path, vocab2idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
